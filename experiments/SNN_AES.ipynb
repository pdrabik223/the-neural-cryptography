{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fd982564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import snntorch as snn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "from aes_commons import *\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4cb697df",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # Meta\n",
    "    \"no_epochs\": 10,\n",
    "    \"batch_size\" : 100000,\n",
    "    # SNN config\n",
    "    \"num_steps\": 1,\n",
    "    \"beta\": 0.95,\n",
    "    \n",
    "    # Network dimentions\n",
    "    \"num_inputs\": 128,\n",
    "    \"num_spiking1\": 128,\n",
    "    \"num_spiking2\": 128,\n",
    "    \"num_hidden_out\": 128,\n",
    "    \"num_outputs\": 128\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6f5700fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def bytes_to_float_array(data_block:List[uint8])->List[float]:\n",
    "    result : List[float] = []\n",
    "    \n",
    "    for id, byte in enumerate(data_block):\n",
    "        input = bin(byte)\n",
    "        input = input[2:]\n",
    "        input = input[::-1]\n",
    "        for _ in range(8):\n",
    "            result.append(0.0)\n",
    "\n",
    "        for i, bit in enumerate(input):\n",
    "            result[id * 8 + i] = float(bit)\n",
    "            \n",
    "    return result\n",
    "print(bytes_to_float_array({uint8(6)}))\n",
    "\n",
    "@dataclass\n",
    "class CryptoDataset:\n",
    "\n",
    "    no_bytes: int # no of bytes in ciphered message\n",
    "    batch_size: int\n",
    "    n :int\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_config():        \n",
    "        return CryptoDataset(no_bytes=config[\"num_inputs\"]//8,batch_size = config[\"batch_size\"])\n",
    "        \n",
    "    def __init__(self,  no_bytes:int, batch_size:int):\n",
    "        self.no_bytes = no_bytes\n",
    "        self.batch_size = batch_size\n",
    "        self.n = 0\n",
    "    \n",
    "    def next_sub_byte(self):\n",
    "        \n",
    "        label : List[uint8] = []\n",
    "        for _ in range(self.no_bytes) :\n",
    "            label.append(uint8(random.randint(0,255)))\n",
    "        \n",
    "        input : List[uint8] = mix_columns(label) \n",
    "        return (torch.FloatTensor(bytes_to_float_array(input)),torch.FloatTensor(bytes_to_float_array(label)))\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.n = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.n < self.batch_size:\n",
    "            self.n += 1\n",
    "            return self.next_sub_byte()\n",
    "        else:\n",
    "            raise StopIteration\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ce62f7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snntorch import spikegen\n",
    "\n",
    "# spike_data = spikegen.rate(data_it, num_steps=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2a0ac995",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__();\n",
    "        # Initialize layers\n",
    "        self.fc0 = nn.Linear(config[\"num_inputs\"], config[\"num_spiking1\"])\n",
    "        self.fc1 = nn.Linear(config[\"num_spiking1\"], config[\"num_spiking2\"])\n",
    "        self.lif1 = snn.Leaky(beta=config[\"beta\"])\n",
    "        self.fc2 = nn.Linear(config[\"num_spiking2\"], config[\"num_hidden_out\"])\n",
    "        self.lif2 = snn.Leaky(beta=config[\"beta\"])\n",
    "        \n",
    "        self.fc3 = nn.Linear(config[\"num_hidden_out\"], config[\"num_outputs\"])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state for each pass\n",
    "        memory1 = self.lif1.init_leaky()\n",
    "        memory2 = self.lif2.init_leaky()\n",
    "        \n",
    "        result_spikes = []\n",
    "        result_membrain = []\n",
    "        y = self.fc0(x)\n",
    "        \n",
    "        x = spikegen.rate(x, num_steps=config[\"num_steps\"])\n",
    "        \n",
    "        spike_sum = torch.zeros(config[\"num_hidden_out\"])\n",
    "        for step in range(config[\"num_steps\"]):\n",
    "            cur1 = self.fc1(y)\n",
    "            spikes1, memory1 = self.lif1(cur1, memory1)\n",
    "            cur2 = self.fc2(spikes1)\n",
    "            spikes2, memory2 = self.lif2(cur2, memory2)\n",
    "            spike_sum += spikes2\n",
    "        \n",
    "        out = self.fc3(spike_sum)\n",
    "        \n",
    "        # Maybe another dense layer\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8ecb6c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e3b44552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CryptoDataset(no_bytes=16, batch_size=100000, n=0)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = CryptoDataset.from_config()\n",
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "507b5638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import snntorch.spikeplot as splt\n",
    "\n",
    "# Iterate through minibatches\n",
    "data = iter(train_loader)\n",
    "data_it, targets_it = next(data)\n",
    "\n",
    "# Spiking Data\n",
    "spike_data =spikegen.rate(data_it, num_steps=config[\"num_steps\"])\n",
    "spike_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b464d522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2030dea1a30>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spike_data = spike_data[:]\n",
    "spike_data_sample_plot = spike_data.reshape((config[\"num_steps\"], -1))\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "splt.raster(spike_data_sample_plot, ax, s=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "73b9a808",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.MSELoss() # Idk\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "loss_hist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "46ff1c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(config[\"no_epochs\"]):\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        net.train()\n",
    "        \n",
    "        # forward\n",
    "        output = net(x)\n",
    "        loss = loss_func(output, y)\n",
    "        \n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "55df64e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2030defbeb0>]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "98b52489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    output: [0.48751572 0.5034663  0.5157932  0.5176997  0.5008864  0.50126183\n",
      " 0.48557192 0.49512383 0.499977   0.4998512  0.49099678 0.50653255\n",
      " 0.5205765  0.5087731  0.49183303 0.49338132 0.4952267  0.5079662\n",
      " 0.47319168 0.49238405 0.4779486  0.4943228  0.51486856 0.49120367\n",
      " 0.49489436 0.49199587 0.49770096 0.48714966 0.50179034 0.5033951\n",
      " 0.5100289  0.49880522 0.50779825 0.49366772 0.5116118  0.50248235\n",
      " 0.50042254 0.5029278  0.49758875 0.49819162 0.50939196 0.512046\n",
      " 0.5049743  0.5246529  0.5097925  0.5065173  0.50206095 0.4792766\n",
      " 0.5060841  0.5025823  0.5061633  0.48903894 0.49269757 0.5028648\n",
      " 0.4937952  0.49568272 0.51971024 0.4778083  0.5017931  0.52415156\n",
      " 0.48643178 0.5105267  0.48039535 0.514869   0.49042565 0.49070135\n",
      " 0.5166881  0.5162278  0.50267714 0.5205514  0.5329444  0.5121941\n",
      " 0.5064275  0.4799664  0.4995077  0.4912384  0.4928809  0.50565404\n",
      " 0.48045522 0.47930762 0.50418353 0.4995076  0.49596035 0.48549798\n",
      " 0.5069944  0.5170609  0.51446784 0.5052806  0.50782907 0.4863727\n",
      " 0.51268905 0.50699    0.47738397 0.47885698 0.4861179  0.49407244\n",
      " 0.49458903 0.49087366 0.51192415 0.49646416 0.50080365 0.50553393\n",
      " 0.48024908 0.49108592 0.4908152  0.51183623 0.49275386 0.5003087\n",
      " 0.50686514 0.49247944 0.4904068  0.4886835  0.4944382  0.5003084\n",
      " 0.4818353  0.5065694  0.49680358 0.48312068 0.4910327  0.50051767\n",
      " 0.50292987 0.5093559  0.50291586 0.49760455 0.50707734 0.49463698\n",
      " 0.4925919  0.47221416]\n",
      "rad output: [0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1.\n",
      " 1. 1. 1. 0. 1. 0. 0. 0.]\n",
      "     labels: [0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0.\n",
      " 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "    output: [0.48751572 0.5034663  0.5157932  0.5176997  0.5008864  0.50126183\n",
      " 0.48557192 0.49512383 0.499977   0.4998512  0.49099678 0.50653255\n",
      " 0.5205765  0.5087731  0.49183303 0.49338132 0.4952267  0.5079662\n",
      " 0.47319168 0.49238405 0.4779486  0.4943228  0.51486856 0.49120367\n",
      " 0.49489436 0.49199587 0.49770096 0.48714966 0.50179034 0.5033951\n",
      " 0.5100289  0.49880522 0.50779825 0.49366772 0.5116118  0.50248235\n",
      " 0.50042254 0.5029278  0.49758875 0.49819162 0.50939196 0.512046\n",
      " 0.5049743  0.5246529  0.5097925  0.5065173  0.50206095 0.4792766\n",
      " 0.5060841  0.5025823  0.5061633  0.48903894 0.49269757 0.5028648\n",
      " 0.4937952  0.49568272 0.51971024 0.4778083  0.5017931  0.52415156\n",
      " 0.48643178 0.5105267  0.48039535 0.514869   0.49042565 0.49070135\n",
      " 0.5166881  0.5162278  0.50267714 0.5205514  0.5329444  0.5121941\n",
      " 0.5064275  0.4799664  0.4995077  0.4912384  0.4928809  0.50565404\n",
      " 0.48045522 0.47930762 0.50418353 0.4995076  0.49596035 0.48549798\n",
      " 0.5069944  0.5170609  0.51446784 0.5052806  0.50782907 0.4863727\n",
      " 0.51268905 0.50699    0.47738397 0.47885698 0.4861179  0.49407244\n",
      " 0.49458903 0.49087366 0.51192415 0.49646416 0.50080365 0.50553393\n",
      " 0.48024908 0.49108592 0.4908152  0.51183623 0.49275386 0.5003087\n",
      " 0.50686514 0.49247944 0.4904068  0.4886835  0.4944382  0.5003084\n",
      " 0.4818353  0.5065694  0.49680358 0.48312068 0.4910327  0.50051767\n",
      " 0.50292987 0.5093559  0.50291586 0.49760455 0.50707734 0.49463698\n",
      " 0.4925919  0.47221416]\n",
      "rad output: [0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1.\n",
      " 1. 1. 1. 0. 1. 0. 0. 0.]\n",
      "     labels: [1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0.\n",
      " 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1.\n",
      " 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 1. 1.]\n",
      "    output: [0.48751572 0.5034663  0.5157932  0.5176997  0.5008864  0.50126183\n",
      " 0.48557192 0.49512383 0.499977   0.4998512  0.49099678 0.50653255\n",
      " 0.5205765  0.5087731  0.49183303 0.49338132 0.4952267  0.5079662\n",
      " 0.47319168 0.49238405 0.4779486  0.4943228  0.51486856 0.49120367\n",
      " 0.49489436 0.49199587 0.49770096 0.48714966 0.50179034 0.5033951\n",
      " 0.5100289  0.49880522 0.50779825 0.49366772 0.5116118  0.50248235\n",
      " 0.50042254 0.5029278  0.49758875 0.49819162 0.50939196 0.512046\n",
      " 0.5049743  0.5246529  0.5097925  0.5065173  0.50206095 0.4792766\n",
      " 0.5060841  0.5025823  0.5061633  0.48903894 0.49269757 0.5028648\n",
      " 0.4937952  0.49568272 0.51971024 0.4778083  0.5017931  0.52415156\n",
      " 0.48643178 0.5105267  0.48039535 0.514869   0.49042565 0.49070135\n",
      " 0.5166881  0.5162278  0.50267714 0.5205514  0.5329444  0.5121941\n",
      " 0.5064275  0.4799664  0.4995077  0.4912384  0.4928809  0.50565404\n",
      " 0.48045522 0.47930762 0.50418353 0.4995076  0.49596035 0.48549798\n",
      " 0.5069944  0.5170609  0.51446784 0.5052806  0.50782907 0.4863727\n",
      " 0.51268905 0.50699    0.47738397 0.47885698 0.4861179  0.49407244\n",
      " 0.49458903 0.49087366 0.51192415 0.49646416 0.50080365 0.50553393\n",
      " 0.48024908 0.49108592 0.4908152  0.51183623 0.49275386 0.5003087\n",
      " 0.50686514 0.49247944 0.4904068  0.4886835  0.4944382  0.5003084\n",
      " 0.4818353  0.5065694  0.49680358 0.48312068 0.4910327  0.50051767\n",
      " 0.50292987 0.5093559  0.50291586 0.49760455 0.50707734 0.49463698\n",
      " 0.4925919  0.47221416]\n",
      "rad output: [0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1.\n",
      " 1. 1. 1. 0. 1. 0. 0. 0.]\n",
      "     labels: [1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0.\n",
      " 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "    output: [0.48751572 0.5034663  0.5157932  0.5176997  0.5008864  0.50126183\n",
      " 0.48557192 0.49512383 0.499977   0.4998512  0.49099678 0.50653255\n",
      " 0.5205765  0.5087731  0.49183303 0.49338132 0.4952267  0.5079662\n",
      " 0.47319168 0.49238405 0.4779486  0.4943228  0.51486856 0.49120367\n",
      " 0.49489436 0.49199587 0.49770096 0.48714966 0.50179034 0.5033951\n",
      " 0.5100289  0.49880522 0.50779825 0.49366772 0.5116118  0.50248235\n",
      " 0.50042254 0.5029278  0.49758875 0.49819162 0.50939196 0.512046\n",
      " 0.5049743  0.5246529  0.5097925  0.5065173  0.50206095 0.4792766\n",
      " 0.5060841  0.5025823  0.5061633  0.48903894 0.49269757 0.5028648\n",
      " 0.4937952  0.49568272 0.51971024 0.4778083  0.5017931  0.52415156\n",
      " 0.48643178 0.5105267  0.48039535 0.514869   0.49042565 0.49070135\n",
      " 0.5166881  0.5162278  0.50267714 0.5205514  0.5329444  0.5121941\n",
      " 0.5064275  0.4799664  0.4995077  0.4912384  0.4928809  0.50565404\n",
      " 0.48045522 0.47930762 0.50418353 0.4995076  0.49596035 0.48549798\n",
      " 0.5069944  0.5170609  0.51446784 0.5052806  0.50782907 0.4863727\n",
      " 0.51268905 0.50699    0.47738397 0.47885698 0.4861179  0.49407244\n",
      " 0.49458903 0.49087366 0.51192415 0.49646416 0.50080365 0.50553393\n",
      " 0.48024908 0.49108592 0.4908152  0.51183623 0.49275386 0.5003087\n",
      " 0.50686514 0.49247944 0.4904068  0.4886835  0.4944382  0.5003084\n",
      " 0.4818353  0.5065694  0.49680358 0.48312068 0.4910327  0.50051767\n",
      " 0.50292987 0.5093559  0.50291586 0.49760455 0.50707734 0.49463698\n",
      " 0.4925919  0.47221416]\n",
      "rad output: [0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1.\n",
      " 1. 1. 1. 0. 1. 0. 0. 0.]\n",
      "     labels: [0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0.\n",
      " 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1.\n",
      " 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1.\n",
      " 0. 0. 1. 0. 1. 0. 1. 1.]\n",
      "    output: [0.48751572 0.5034663  0.5157932  0.5176997  0.5008864  0.50126183\n",
      " 0.48557192 0.49512383 0.499977   0.4998512  0.49099678 0.50653255\n",
      " 0.5205765  0.5087731  0.49183303 0.49338132 0.4952267  0.5079662\n",
      " 0.47319168 0.49238405 0.4779486  0.4943228  0.51486856 0.49120367\n",
      " 0.49489436 0.49199587 0.49770096 0.48714966 0.50179034 0.5033951\n",
      " 0.5100289  0.49880522 0.50779825 0.49366772 0.5116118  0.50248235\n",
      " 0.50042254 0.5029278  0.49758875 0.49819162 0.50939196 0.512046\n",
      " 0.5049743  0.5246529  0.5097925  0.5065173  0.50206095 0.4792766\n",
      " 0.5060841  0.5025823  0.5061633  0.48903894 0.49269757 0.5028648\n",
      " 0.4937952  0.49568272 0.51971024 0.4778083  0.5017931  0.52415156\n",
      " 0.48643178 0.5105267  0.48039535 0.514869   0.49042565 0.49070135\n",
      " 0.5166881  0.5162278  0.50267714 0.5205514  0.5329444  0.5121941\n",
      " 0.5064275  0.4799664  0.4995077  0.4912384  0.4928809  0.50565404\n",
      " 0.48045522 0.47930762 0.50418353 0.4995076  0.49596035 0.48549798\n",
      " 0.5069944  0.5170609  0.51446784 0.5052806  0.50782907 0.4863727\n",
      " 0.51268905 0.50699    0.47738397 0.47885698 0.4861179  0.49407244\n",
      " 0.49458903 0.49087366 0.51192415 0.49646416 0.50080365 0.50553393\n",
      " 0.48024908 0.49108592 0.4908152  0.51183623 0.49275386 0.5003087\n",
      " 0.50686514 0.49247944 0.4904068  0.4886835  0.4944382  0.5003084\n",
      " 0.4818353  0.5065694  0.49680358 0.48312068 0.4910327  0.50051767\n",
      " 0.50292987 0.5093559  0.50291586 0.49760455 0.50707734 0.49463698\n",
      " 0.4925919  0.47221416]\n",
      "rad output: [0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1.\n",
      " 1. 1. 1. 0. 1. 0. 0. 0.]\n",
      "     labels: [1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1.\n",
      " 0. 1. 0. 1. 0. 0. 1. 1.]\n",
      "    output: [0.48751572 0.5034663  0.5157932  0.5176997  0.5008864  0.50126183\n",
      " 0.48557192 0.49512383 0.499977   0.4998512  0.49099678 0.50653255\n",
      " 0.5205765  0.5087731  0.49183303 0.49338132 0.4952267  0.5079662\n",
      " 0.47319168 0.49238405 0.4779486  0.4943228  0.51486856 0.49120367\n",
      " 0.49489436 0.49199587 0.49770096 0.48714966 0.50179034 0.5033951\n",
      " 0.5100289  0.49880522 0.50779825 0.49366772 0.5116118  0.50248235\n",
      " 0.50042254 0.5029278  0.49758875 0.49819162 0.50939196 0.512046\n",
      " 0.5049743  0.5246529  0.5097925  0.5065173  0.50206095 0.4792766\n",
      " 0.5060841  0.5025823  0.5061633  0.48903894 0.49269757 0.5028648\n",
      " 0.4937952  0.49568272 0.51971024 0.4778083  0.5017931  0.52415156\n",
      " 0.48643178 0.5105267  0.48039535 0.514869   0.49042565 0.49070135\n",
      " 0.5166881  0.5162278  0.50267714 0.5205514  0.5329444  0.5121941\n",
      " 0.5064275  0.4799664  0.4995077  0.4912384  0.4928809  0.50565404\n",
      " 0.48045522 0.47930762 0.50418353 0.4995076  0.49596035 0.48549798\n",
      " 0.5069944  0.5170609  0.51446784 0.5052806  0.50782907 0.4863727\n",
      " 0.51268905 0.50699    0.47738397 0.47885698 0.4861179  0.49407244\n",
      " 0.49458903 0.49087366 0.51192415 0.49646416 0.50080365 0.50553393\n",
      " 0.48024908 0.49108592 0.4908152  0.51183623 0.49275386 0.5003087\n",
      " 0.50686514 0.49247944 0.4904068  0.4886835  0.4944382  0.5003084\n",
      " 0.4818353  0.5065694  0.49680358 0.48312068 0.4910327  0.50051767\n",
      " 0.50292987 0.5093559  0.50291586 0.49760455 0.50707734 0.49463698\n",
      " 0.4925919  0.47221416]\n",
      "rad output: [0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1.\n",
      " 1. 1. 1. 0. 1. 0. 0. 0.]\n",
      "     labels: [0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0.\n",
      " 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1.\n",
      " 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "    output: [0.48751572 0.5034663  0.5157932  0.5176997  0.5008864  0.50126183\n",
      " 0.48557192 0.49512383 0.499977   0.4998512  0.49099678 0.50653255\n",
      " 0.5205765  0.5087731  0.49183303 0.49338132 0.4952267  0.5079662\n",
      " 0.47319168 0.49238405 0.4779486  0.4943228  0.51486856 0.49120367\n",
      " 0.49489436 0.49199587 0.49770096 0.48714966 0.50179034 0.5033951\n",
      " 0.5100289  0.49880522 0.50779825 0.49366772 0.5116118  0.50248235\n",
      " 0.50042254 0.5029278  0.49758875 0.49819162 0.50939196 0.512046\n",
      " 0.5049743  0.5246529  0.5097925  0.5065173  0.50206095 0.4792766\n",
      " 0.5060841  0.5025823  0.5061633  0.48903894 0.49269757 0.5028648\n",
      " 0.4937952  0.49568272 0.51971024 0.4778083  0.5017931  0.52415156\n",
      " 0.48643178 0.5105267  0.48039535 0.514869   0.49042565 0.49070135\n",
      " 0.5166881  0.5162278  0.50267714 0.5205514  0.5329444  0.5121941\n",
      " 0.5064275  0.4799664  0.4995077  0.4912384  0.4928809  0.50565404\n",
      " 0.48045522 0.47930762 0.50418353 0.4995076  0.49596035 0.48549798\n",
      " 0.5069944  0.5170609  0.51446784 0.5052806  0.50782907 0.4863727\n",
      " 0.51268905 0.50699    0.47738397 0.47885698 0.4861179  0.49407244\n",
      " 0.49458903 0.49087366 0.51192415 0.49646416 0.50080365 0.50553393\n",
      " 0.48024908 0.49108592 0.4908152  0.51183623 0.49275386 0.5003087\n",
      " 0.50686514 0.49247944 0.4904068  0.4886835  0.4944382  0.5003084\n",
      " 0.4818353  0.5065694  0.49680358 0.48312068 0.4910327  0.50051767\n",
      " 0.50292987 0.5093559  0.50291586 0.49760455 0.50707734 0.49463698\n",
      " 0.4925919  0.47221416]\n",
      "rad output: [0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1.\n",
      " 1. 1. 1. 0. 1. 0. 0. 0.]\n",
      "     labels: [0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1.\n",
      " 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1.\n",
      " 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.\n",
      " 1. 0. 1. 0. 0. 0. 1. 0.]\n",
      "    output: [0.48751572 0.5034663  0.5157932  0.5176997  0.5008864  0.50126183\n",
      " 0.48557192 0.49512383 0.499977   0.4998512  0.49099678 0.50653255\n",
      " 0.5205765  0.5087731  0.49183303 0.49338132 0.4952267  0.5079662\n",
      " 0.47319168 0.49238405 0.4779486  0.4943228  0.51486856 0.49120367\n",
      " 0.49489436 0.49199587 0.49770096 0.48714966 0.50179034 0.5033951\n",
      " 0.5100289  0.49880522 0.50779825 0.49366772 0.5116118  0.50248235\n",
      " 0.50042254 0.5029278  0.49758875 0.49819162 0.50939196 0.512046\n",
      " 0.5049743  0.5246529  0.5097925  0.5065173  0.50206095 0.4792766\n",
      " 0.5060841  0.5025823  0.5061633  0.48903894 0.49269757 0.5028648\n",
      " 0.4937952  0.49568272 0.51971024 0.4778083  0.5017931  0.52415156\n",
      " 0.48643178 0.5105267  0.48039535 0.514869   0.49042565 0.49070135\n",
      " 0.5166881  0.5162278  0.50267714 0.5205514  0.5329444  0.5121941\n",
      " 0.5064275  0.4799664  0.4995077  0.4912384  0.4928809  0.50565404\n",
      " 0.48045522 0.47930762 0.50418353 0.4995076  0.49596035 0.48549798\n",
      " 0.5069944  0.5170609  0.51446784 0.5052806  0.50782907 0.4863727\n",
      " 0.51268905 0.50699    0.47738397 0.47885698 0.4861179  0.49407244\n",
      " 0.49458903 0.49087366 0.51192415 0.49646416 0.50080365 0.50553393\n",
      " 0.48024908 0.49108592 0.4908152  0.51183623 0.49275386 0.5003087\n",
      " 0.50686514 0.49247944 0.4904068  0.4886835  0.4944382  0.5003084\n",
      " 0.4818353  0.5065694  0.49680358 0.48312068 0.4910327  0.50051767\n",
      " 0.50292987 0.5093559  0.50291586 0.49760455 0.50707734 0.49463698\n",
      " 0.4925919  0.47221416]\n",
      "rad output: [0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1.\n",
      " 1. 1. 1. 0. 1. 0. 0. 0.]\n",
      "     labels: [0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 1. 0. 1.]\n",
      "    output: [0.48751572 0.5034663  0.5157932  0.5176997  0.5008864  0.50126183\n",
      " 0.48557192 0.49512383 0.499977   0.4998512  0.49099678 0.50653255\n",
      " 0.5205765  0.5087731  0.49183303 0.49338132 0.4952267  0.5079662\n",
      " 0.47319168 0.49238405 0.4779486  0.4943228  0.51486856 0.49120367\n",
      " 0.49489436 0.49199587 0.49770096 0.48714966 0.50179034 0.5033951\n",
      " 0.5100289  0.49880522 0.50779825 0.49366772 0.5116118  0.50248235\n",
      " 0.50042254 0.5029278  0.49758875 0.49819162 0.50939196 0.512046\n",
      " 0.5049743  0.5246529  0.5097925  0.5065173  0.50206095 0.4792766\n",
      " 0.5060841  0.5025823  0.5061633  0.48903894 0.49269757 0.5028648\n",
      " 0.4937952  0.49568272 0.51971024 0.4778083  0.5017931  0.52415156\n",
      " 0.48643178 0.5105267  0.48039535 0.514869   0.49042565 0.49070135\n",
      " 0.5166881  0.5162278  0.50267714 0.5205514  0.5329444  0.5121941\n",
      " 0.5064275  0.4799664  0.4995077  0.4912384  0.4928809  0.50565404\n",
      " 0.48045522 0.47930762 0.50418353 0.4995076  0.49596035 0.48549798\n",
      " 0.5069944  0.5170609  0.51446784 0.5052806  0.50782907 0.4863727\n",
      " 0.51268905 0.50699    0.47738397 0.47885698 0.4861179  0.49407244\n",
      " 0.49458903 0.49087366 0.51192415 0.49646416 0.50080365 0.50553393\n",
      " 0.48024908 0.49108592 0.4908152  0.51183623 0.49275386 0.5003087\n",
      " 0.50686514 0.49247944 0.4904068  0.4886835  0.4944382  0.5003084\n",
      " 0.4818353  0.5065694  0.49680358 0.48312068 0.4910327  0.50051767\n",
      " 0.50292987 0.5093559  0.50291586 0.49760455 0.50707734 0.49463698\n",
      " 0.4925919  0.47221416]\n",
      "rad output: [0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1.\n",
      " 1. 1. 1. 0. 1. 0. 0. 0.]\n",
      "     labels: [0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0.\n",
      " 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0.\n",
      " 0. 1. 0. 1. 0. 0. 1. 0.]\n",
      "    output: [0.48751572 0.5034663  0.5157932  0.5176997  0.5008864  0.50126183\n",
      " 0.48557192 0.49512383 0.499977   0.4998512  0.49099678 0.50653255\n",
      " 0.5205765  0.5087731  0.49183303 0.49338132 0.4952267  0.5079662\n",
      " 0.47319168 0.49238405 0.4779486  0.4943228  0.51486856 0.49120367\n",
      " 0.49489436 0.49199587 0.49770096 0.48714966 0.50179034 0.5033951\n",
      " 0.5100289  0.49880522 0.50779825 0.49366772 0.5116118  0.50248235\n",
      " 0.50042254 0.5029278  0.49758875 0.49819162 0.50939196 0.512046\n",
      " 0.5049743  0.5246529  0.5097925  0.5065173  0.50206095 0.4792766\n",
      " 0.5060841  0.5025823  0.5061633  0.48903894 0.49269757 0.5028648\n",
      " 0.4937952  0.49568272 0.51971024 0.4778083  0.5017931  0.52415156\n",
      " 0.48643178 0.5105267  0.48039535 0.514869   0.49042565 0.49070135\n",
      " 0.5166881  0.5162278  0.50267714 0.5205514  0.5329444  0.5121941\n",
      " 0.5064275  0.4799664  0.4995077  0.4912384  0.4928809  0.50565404\n",
      " 0.48045522 0.47930762 0.50418353 0.4995076  0.49596035 0.48549798\n",
      " 0.5069944  0.5170609  0.51446784 0.5052806  0.50782907 0.4863727\n",
      " 0.51268905 0.50699    0.47738397 0.47885698 0.4861179  0.49407244\n",
      " 0.49458903 0.49087366 0.51192415 0.49646416 0.50080365 0.50553393\n",
      " 0.48024908 0.49108592 0.4908152  0.51183623 0.49275386 0.5003087\n",
      " 0.50686514 0.49247944 0.4904068  0.4886835  0.4944382  0.5003084\n",
      " 0.4818353  0.5065694  0.49680358 0.48312068 0.4910327  0.50051767\n",
      " 0.50292987 0.5093559  0.50291586 0.49760455 0.50707734 0.49463698\n",
      " 0.4925919  0.47221416]\n",
      "rad output: [0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1.\n",
      " 1. 1. 1. 0. 1. 0. 0. 0.]\n",
      "     labels: [1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1.\n",
      " 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1.\n",
      " 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1.\n",
      " 0. 0. 1. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "def diff(output:List[float],label:List[float]):\n",
    "\n",
    "        same = 0.0\n",
    "        for o, l in zip(output,label):\n",
    "            same += abs(o - l)\n",
    "\n",
    "        return same / len(output)\n",
    "def radical(output:List[float])->List[int]:\n",
    "    solution = np.zeros(len(output))\n",
    "    for id, o in enumerate(output):\n",
    "        if o >= 0.5:\n",
    "            solution[id] = 1\n",
    "        else:\n",
    "            solution[id] = 0\n",
    "    return solution\n",
    "\n",
    "global_difference = 0\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for index in range(10):\n",
    "        inputs, labels = train_loader.next_sub_byte() # torch.tensor([float(i)]) / 100, torch.tensor([float(i+10)]) / 100\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        print(f\"    output: {outputs.detach().numpy()}\\nrad output: {radical(outputs.detach().numpy())}\\n     labels: {radical(labels)}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "cef7d6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128])\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'label' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\piotr\\Documents\\the-neural-cryptography\\experiments\\SNN_AES.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/piotr/Documents/the-neural-cryptography/experiments/SNN_AES.ipynb#ch0000016?line=34'>35</a>\u001b[0m         \u001b[39minput\u001b[39m, label \u001b[39m=\u001b[39m train_loader\u001b[39m.\u001b[39mnext_sub_byte()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/piotr/Documents/the-neural-cryptography/experiments/SNN_AES.ipynb#ch0000016?line=35'>36</a>\u001b[0m         output \u001b[39m=\u001b[39m net(\u001b[39minput\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/piotr/Documents/the-neural-cryptography/experiments/SNN_AES.ipynb#ch0000016?line=36'>37</a>\u001b[0m         global_diff \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_func(label,output)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/piotr/Documents/the-neural-cryptography/experiments/SNN_AES.ipynb#ch0000016?line=38'>39</a>\u001b[0m \u001b[39mprint\u001b[39m(global_diff \u001b[39m/\u001b[39m\u001b[39m10_00\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\piotr\\Documents\\the-neural-cryptography\\experiments\\SNN_AES.ipynb Cell 14'\u001b[0m in \u001b[0;36mloss_func\u001b[1;34m(input, output)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/piotr/Documents/the-neural-cryptography/experiments/SNN_AES.ipynb#ch0000016?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss_func\u001b[39m(\u001b[39minput\u001b[39m, output):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/piotr/Documents/the-neural-cryptography/experiments/SNN_AES.ipynb#ch0000016?line=12'>13</a>\u001b[0m     output \u001b[39m=\u001b[39m radical(output\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/piotr/Documents/the-neural-cryptography/experiments/SNN_AES.ipynb#ch0000016?line=13'>14</a>\u001b[0m     label \u001b[39m=\u001b[39m radical(label\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/piotr/Documents/the-neural-cryptography/experiments/SNN_AES.ipynb#ch0000016?line=14'>15</a>\u001b[0m     same \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/piotr/Documents/the-neural-cryptography/experiments/SNN_AES.ipynb#ch0000016?line=15'>16</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i, o \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39minput\u001b[39m, output):\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'label' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# def loss_func(input, output):\n",
    "#     output = radical(output)\n",
    "#     input = radical(input)\n",
    "#     same = 0\n",
    "#     for i, o in zip(input, output):\n",
    "#         if i == o:\n",
    "#             same += 1\n",
    "#         else:\n",
    "#             break\n",
    "#     return torch.tensor(1 - (same / len(input)), requires_grad=True)\n",
    "\n",
    "def loss_func(input, output):\n",
    "    output = radical(output.detach().numpy())\n",
    "    label = radical(label.detach().numpy())\n",
    "    same = 0\n",
    "    for i, o in zip(input, output):\n",
    "        if i == o:\n",
    "            same += 1\n",
    "        else:\n",
    "            break\n",
    "    return same \n",
    "\n",
    "def radical_diff(output:List[float],label:List[float]):\n",
    "    output = radical(output)\n",
    "    label = radical(label)\n",
    "    difference = 0\n",
    "    for o,l in zip(output,label):\n",
    "        if o != l:\n",
    "            difference+=1\n",
    "    return difference\n",
    "\n",
    "global_diff = 0\n",
    "print(output.shape)\n",
    "for _ in range(10_00):\n",
    "        input, label = train_loader.next_sub_byte()\n",
    "        output = net(input)\n",
    "        global_diff += loss_func(label,output)\n",
    "        \n",
    "print(global_diff /10_00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d77822b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cdface",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "372283a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, got 128, 128x128,1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\piotr\\Documents\\the-neural-cryptography\\experiments\\SNN_AES.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/piotr/Documents/the-neural-cryptography/experiments/SNN_AES.ipynb#ch0000012?line=0'>1</a>\u001b[0m net(torch\u001b[39m.\u001b[39;49mtensor([\u001b[39m1\u001b[39;49m], dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32))\n",
      "File \u001b[1;32mc:\\I\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\piotr\\Documents\\the-neural-cryptography\\experiments\\SNN_AES.ipynb Cell 5'\u001b[0m in \u001b[0;36mModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/piotr/Documents/the-neural-cryptography/experiments/SNN_AES.ipynb#ch0000004?line=17'>18</a>\u001b[0m result_spikes \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/piotr/Documents/the-neural-cryptography/experiments/SNN_AES.ipynb#ch0000004?line=18'>19</a>\u001b[0m result_membrain \u001b[39m=\u001b[39m []\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/piotr/Documents/the-neural-cryptography/experiments/SNN_AES.ipynb#ch0000004?line=19'>20</a>\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc0(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/piotr/Documents/the-neural-cryptography/experiments/SNN_AES.ipynb#ch0000004?line=21'>22</a>\u001b[0m x \u001b[39m=\u001b[39m spikegen\u001b[39m.\u001b[39mrate(x, num_steps\u001b[39m=\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mnum_steps\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/piotr/Documents/the-neural-cryptography/experiments/SNN_AES.ipynb#ch0000004?line=23'>24</a>\u001b[0m spike_sum \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(config[\u001b[39m\"\u001b[39m\u001b[39mnum_hidden_out\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\I\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\I\\python38\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: size mismatch, got 128, 128x128,1"
     ]
    }
   ],
   "source": [
    "net(torch.tensor([1], dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5656b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8rc1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8rc1"
  },
  "vscode": {
   "interpreter": {
    "hash": "5b3521386b79ae8a71fb6b122c8060f20d899f82a7bb4ecd5817dc5576419c14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
