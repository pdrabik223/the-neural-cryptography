{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8350b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "\n",
    "# iv - initialisation vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d883f4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "batch_size=128\n",
    "data_path='./data/mnist'\n",
    "num_classes = 10  # MNIST has 10 output classes\n",
    "\n",
    "# Torch Variables\n",
    "dtype = torch.float\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4615b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((28,28)),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5331e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snntorch import utils\n",
    "\n",
    "subset = 10\n",
    "# mnist_train = utils.data_subset(mnist_train, subset)\n",
    "len(mnist_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78a5166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2d1f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal Dynamics\n",
    "num_steps = 10\n",
    "\n",
    "# create vector filled with 0.5\n",
    "raw_vector = torch.ones(num_steps)*0.5\n",
    "\n",
    "# pass each sample through a Bernoulli trial\n",
    "rate_coded_vector = torch.bernoulli(raw_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de1becc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The output is spiking {rate_coded_vector.sum()*100/len(rate_coded_vector):.2f}% of the time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c9dac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 100\n",
    "\n",
    "# create vector filled with 0.5\n",
    "raw_vector = torch.ones(num_steps)*0.5\n",
    "\n",
    "# pass each sample through a Bernoulli trial\n",
    "rate_coded_vector = torch.bernoulli(raw_vector)\n",
    "print(f\"The output is spiking {rate_coded_vector.sum()*100/len(rate_coded_vector):.2f}% of the time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0004e862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snntorch import spikegen\n",
    "\n",
    "# Iterate through minibatches\n",
    "data = iter(train_loader)\n",
    "data_it, targets_it = next(data)\n",
    "\n",
    "# Spiking Data\n",
    "spike_data = spikegen.rate(data_it, num_steps=num_steps)\n",
    "spike_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ff6344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c107fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_data_sample = spike_data[:, 0, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469c739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_data_sample_plot = spike_data_sample.reshape((num_steps, -1))\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "splt.raster(spike_data_sample_plot, ax, s=1.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a168f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "anim = splt.animator(spike_data_sample, fig, ax)\n",
    "# plt.rcParams['animation.ffmpeg_path'] = 'C:\\\\path\\\\to\\\\your\\\\ffmpeg.exe'\n",
    "\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28be984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = {\n",
    "    # Network Architecture\n",
    "num_inputs = 28*28\n",
    "num_hidden = 1000\n",
    "num_outputs = 10\n",
    "\n",
    "    # Temporal Dynamics\n",
    "num_steps = 25\n",
    "beta = 0.95\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d9de23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize layers\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "\n",
    "        # Record the final layer\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            cur1 = self.fc1(x)\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
    "\n",
    "# Load the network onto CUDA if available\n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789edfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass data into the network, sum the spikes over time\n",
    "# and compare the neuron with the highest number of spikes\n",
    "# with the target\n",
    "\n",
    "def print_batch_accuracy(data, targets, train=False):\n",
    "    output, _ = net(data.view(batch_size, -1))\n",
    "    _, idx = output.sum(dim=0).max(1)\n",
    "    acc = np.mean((targets == idx).detach().cpu().numpy())\n",
    "\n",
    "    if train:\n",
    "        print(f\"Train set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Test set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "\n",
    "def train_printer():\n",
    "    print(f\"Epoch {epoch}, Iteration {iter_counter}\")\n",
    "    print(f\"Train Set Loss: {loss_hist[counter]:.2f}\")\n",
    "    print(f\"Test Set Loss: {test_loss_hist[counter]:.2f}\")\n",
    "    print_batch_accuracy(data, targets, train=True)\n",
    "    print_batch_accuracy(test_data, test_targets, train=False)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ae8c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline(data):\n",
    "    return torch.mean(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d59e440",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c2f3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "loss_hist = []\n",
    "test_loss_hist = []\n",
    "counter = 0\n",
    "wo_test = True\n",
    "\n",
    "# Outer training loop\n",
    "for epoch in range(num_epochs):\n",
    "    iter_counter = 0\n",
    "    train_batch = iter(train_loader)\n",
    "\n",
    "    # Minibatch training loop\n",
    "    for data, targets in train_batch:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        net.train()\n",
    "        spk_rec, mem_rec = net(data.view(batch_size, -1))\n",
    "\n",
    "        # initialize the loss & sum over time\n",
    "        loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "        for step in range(num_steps):\n",
    "            loss_val += loss(mem_rec[step], targets)\n",
    "\n",
    "        print(\"targets[0]\", targets.detach().numpy()[0])\n",
    "\n",
    "        \n",
    "        print(\"Len spk_rec\", len(spk_rec))\n",
    "        print(\"Len mem_rec\", len(mem_rec))\n",
    "        assert len(spk_rec) == len(mem_rec)\n",
    "        for i in range(25):\n",
    "            plt.plot(spk_rec[i][0].detach().numpy())\n",
    "            plt.plot(mem_rec[i][0].detach().numpy())\n",
    "            plt.show()\n",
    "\n",
    "        print(\"Mean: \", get_baseline(spk_rec))\n",
    "        \n",
    "        plt.plot(mem_rec[0][0].detach().numpy())\n",
    "        plt.show()\n",
    "        \n",
    "        break \n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        if wo_test: continue\n",
    "        # Test set\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            test_data, test_targets = next(iter(test_loader))\n",
    "            test_data = test_data.to(device)\n",
    "            test_targets = test_targets.to(device)\n",
    "\n",
    "            # Test set forward pass\n",
    "            test_spk, test_mem = net(test_data.view(batch_size, -1))\n",
    "\n",
    "            # Test set loss\n",
    "            test_loss = torch.zeros((1), dtype=dtype, device=device)\n",
    "            for step in range(num_steps):\n",
    "                test_loss += loss(test_mem[step], test_targets)\n",
    "            test_loss_hist.append(test_loss.item())\n",
    "\n",
    "            # Print train/test loss/accuracy\n",
    "            if counter % 50 == 0:\n",
    "                train_printer()\n",
    "            counter += 1\n",
    "            iter_counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b1da98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "plt.plot(loss_hist)\n",
    "plt.plot(test_loss_hist)\n",
    "plt.title(\"Loss Curves\")\n",
    "plt.legend([\"Train Loss\", \"Test Loss\"])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f85fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e027b21d8c91cb15f29de7082344af1ba339ca876801dd9d70d2e1b0b84984aa"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
